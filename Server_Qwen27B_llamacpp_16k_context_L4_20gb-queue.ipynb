{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPYUTy06Y9gmJa0IohOQz15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiopauli/Qwen3.5-colab/blob/main/Server_Qwen27B_llamacpp_16k_context_L4_20gb-queue.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Celula abaixo demora 8 minutos para ser conclu√≠da"
      ],
      "metadata": {
        "id": "E564goWkNeir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Build llama.cpp with CUDA and run Qwen3.5-27B (non-thinking mode)\n",
        "!apt-get update -qq && apt-get install -qq -y pciutils build-essential cmake curl libcurl4-openssl-dev > /dev/null 2>&1\n",
        "\n",
        "!git clone --depth 1 https://github.com/ggml-org/llama.cpp 2>/dev/null || echo \"already cloned\"\n",
        "\n",
        "!cmake llama.cpp -B llama.cpp/build \\\n",
        "    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON > /dev/null 2>&1\n",
        "\n",
        "!cmake --build llama.cpp/build --config Release -j$(nproc) --clean-first --target llama-cli llama-server 2>&1 | tail -5\n",
        "\n",
        "!cp llama.cpp/build/bin/llama-* llama.cpp/\n",
        "\n",
        "# Download the model\n",
        "!pip install -q huggingface_hub hf_transfer\n",
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download unsloth/Qwen3.5-27B-GGUF \\\n",
        "    --local-dir unsloth/Qwen3.5-27B-GGUF \\\n",
        "    --include \"*UD-Q4_K_XL*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAd6icy52eED",
        "outputId": "17dd0d1f-767f-4932-ec8d-f4e54ab0b764"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "already cloned\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\n",
            "[100%] Linking CXX executable ../../bin/llama-server\n",
            "[100%] Built target llama-server\n",
            "/bin/bash: line 1: huggingface-cli: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A c√©lula abaixo cria o servidor Llamacpp em background."
      ],
      "metadata": {
        "id": "nwbeV8MTNn0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Run llama-server in the background\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Kill any existing server to free up the port\n",
        "os.system(\"pkill -f llama-server\")\n",
        "time.sleep(2)\n",
        "\n",
        "os.environ[\"LLAMA_CACHE\"] = \"unsloth/Qwen3.5-27B-GGUF\"\n",
        "\n",
        "# Start the server using nohup so it runs in the background\n",
        "server_cmd = \"\"\"\n",
        "nohup ./llama.cpp/llama-server \\\n",
        "    -hf unsloth/Qwen3.5-27B-GGUF:UD-Q4_K_XL \\\n",
        "    --host 127.0.0.1 \\\n",
        "    --port 8081 \\\n",
        "    --ctx-size 16384 \\\n",
        "    -ngl 99 \\\n",
        "    --temp 0.7 \\\n",
        "    --top-p 0.8 \\\n",
        "    --top-k 20 \\\n",
        "    --min-p 0.00 \\\n",
        "    --chat-template-kwargs '{\"enable_thinking\": false}' \\\n",
        "    --cache-type-k q8_0 \\\n",
        "    --cache-type-v q8_0 > llama_server.log 2>&1 &\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting llama-server on port 8081...\")\n",
        "os.system(server_cmd)\n",
        "\n",
        "# Wait for the server to spin up and load the model into VRAM\n",
        "print(\"Waiting for model to load into VRAM (this takes 30-60 seconds)...\")\n",
        "for i in range(600):\n",
        "    try:\n",
        "        import requests\n",
        "        res = requests.get(\"http://127.0.0.1:8081/health\")\n",
        "        if res.status_code == 200:\n",
        "            print(\"\\n‚úÖ llama-server is ready and listening on port 8081!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(2)\n",
        "    print(\".\", end=\"\", flush=True)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Server might not have started correctly. Check llama_server.log:\")\n",
        "    os.system(\"tail -n 20 llama_server.log\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rp-w75DFtw1",
        "outputId": "84e40366-677a-4e81-899c-fa0d09b8cef1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting llama-server on port 8081...\n",
            "Waiting for model to load into VRAM (this takes 30-60 seconds)...\n",
            ".......\n",
            "‚úÖ llama-server is ready and listening on port 8081!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguir, criamos outro servidor para gerar os endpoints da API, tamb√©m em background"
      ],
      "metadata": {
        "id": "kmwz2laJSJfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Install dependencies for FastAPI wrapper\n",
        "!pip install -q fastapi uvicorn pyngrok httpx pydantic nest-asyncio"
      ],
      "metadata": {
        "id": "gxYG3JoJ0yU2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Background FastAPI + Cloudflare Tunnel\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "# 1. Write the FastAPI app to a file\n",
        "fastapi_code = \"\"\"\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import StreamingResponse, JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import httpx\n",
        "\n",
        "app = FastAPI(title=\"Custom FastAPI Wrapper for llama.cpp\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "LLAMA_SERVER_URL = \"http://127.0.0.1:8081\"\n",
        "\n",
        "@app.get(\"/v1/models\")\n",
        "async def get_models():\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.get(f\"{LLAMA_SERVER_URL}/v1/models\")\n",
        "        return response.json()\n",
        "\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "async def chat_completions(request: Request):\n",
        "    payload = await request.json()\n",
        "    is_stream = payload.get(\"stream\", False)\n",
        "\n",
        "    if is_stream:\n",
        "        async def generate():\n",
        "            async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "                async with client.stream(\"POST\", f\"{LLAMA_SERVER_URL}/v1/chat/completions\", json=payload) as response:\n",
        "                    async for chunk in response.aiter_bytes():\n",
        "                        yield chunk\n",
        "\n",
        "        return StreamingResponse(generate(), media_type=\"text/event-stream\")\n",
        "    else:\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(f\"{LLAMA_SERVER_URL}/v1/chat/completions\", json=payload)\n",
        "            return JSONResponse(content=response.json(), status_code=response.status_code)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"fastapi_server.py\", \"w\") as f:\n",
        "    f.write(fastapi_code)\n",
        "\n",
        "# 2. Kill existing processes (if you run this cell multiple times)\n",
        "os.system(\"pkill -f uvicorn\")\n",
        "os.system(\"pkill -f cloudflared\")\n",
        "time.sleep(1)\n",
        "\n",
        "# 3. Download Cloudflare if needed\n",
        "if not os.path.exists(\"cloudflared\"):\n",
        "    os.system(\"wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\")\n",
        "    os.system(\"chmod +x cloudflared\")\n",
        "\n",
        "# 4. Start FastAPI in the background via Uvicorn\n",
        "print(\"Starting FastAPI server in the background...\")\n",
        "os.system(\"nohup python -m uvicorn fastapi_server:app --host 0.0.0.0 --port 8000 > fastapi.log 2>&1 &\")\n",
        "\n",
        "# 5. Start Cloudflare Tunnel in the background\n",
        "print(\"Starting Cloudflare Tunnel...\")\n",
        "os.system(\"nohup ./cloudflared tunnel --url http://127.0.0.1:8000 > cloudflare.log 2>&1 &\")\n",
        "\n",
        "# Wait a few seconds for Cloudflare to assign a URL\n",
        "print(\"Waiting for URL...\")\n",
        "time.sleep(8)\n",
        "\n",
        "# 6. Read the log to extract the URL\n",
        "with open(\"cloudflare.log\", \"r\") as f:\n",
        "    logs = f.read()\n",
        "    match = re.search(r\"(https://[a-zA-Z0-9-]+\\.trycloudflare\\.com)\", logs)\n",
        "\n",
        "    if match:\n",
        "        public_url = match.group(1)\n",
        "        base_url = f\"{public_url}/v1\"\n",
        "\n",
        "        # Save the URL to a file\n",
        "        with open(\"api_url.txt\", \"w\") as url_file:\n",
        "            url_file.write(base_url)\n",
        "\n",
        "        print(f\"\\n‚úÖ URL saved to api_url.txt\")\n",
        "        print(f\"üëâ {base_url}\\n\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not find Cloudflare URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObklTeZdLE3i",
        "outputId": "1d2bb929-8df9-42af-ff69-5a6a2b3cde68"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FastAPI server in the background...\n",
            "Starting Cloudflare Tunnel...\n",
            "Waiting for URL...\n",
            "\n",
            "‚úÖ URL saved to api_url.txt\n",
            "üëâ https://editorial-details-updating-turns.trycloudflare.com/v1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo est√° um exemplo de uso da API, pode ser usado de qualquer computador, basta preencher o API_BASE_URL com a URL do servidor da c√©lula acima"
      ],
      "metadata": {
        "id": "9aoJa3MgSadk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test your API with the official OpenAI Python package\n",
        "from openai import OpenAI\n",
        "\n",
        "# Read the base URL automatically from the file\n",
        "with open(\"api_url.txt\", \"r\") as f:\n",
        "    API_BASE_URL = f.read().strip()\n",
        "\n",
        "print(f\"Connecting to: {API_BASE_URL}\\n\")\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=API_BASE_URL,\n",
        "    api_key=\"sk-no-key-required\"\n",
        ")\n",
        "\n",
        "\n",
        "# --- 1. GET MODELS ---\n",
        "print(\"Fetching models...\")\n",
        "models = client.models.list()\n",
        "print(f\"Available models: {[m.id for m in models.data]}\\n\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 2. STREAMING COMPLETION ---\n",
        "print(\"Sending chat request (Streaming)...\\n\")\n",
        "stream_response = client.chat.completions.create(\n",
        "    model=\"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explique o que √© um llamacpp server e o que √© um Cloudflared tunnel\"}\n",
        "    ],\n",
        "    stream=True # <--- Set to True\n",
        ")\n",
        "\n",
        "# Print the streaming response as it arrives\n",
        "for chunk in stream_response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\n\" + \"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 3. NON-STREAMING COMPLETION ---\n",
        "print(\"Sending chat request (Non-Streaming)...\\n\")\n",
        "standard_response = client.chat.completions.create(\n",
        "    model=\"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"O que √© aux√≠lio-doen√ßa no direito brasileiro ? N√£o use markdown na resposta\"}\n",
        "    ],\n",
        "    stream=False # <--- Set to False\n",
        ")\n",
        "\n",
        "# Print the final complete message\n",
        "print(standard_response.choices[0].message.content)\n",
        "print(\"\\n\" + \"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nundtfm5LMFi",
        "outputId": "1cf5f2b2-47d1-4fea-ae7b-e8c3638e9665"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to: https://editorial-details-updating-turns.trycloudflare.com/v1\n",
            "\n",
            "Fetching models...\n",
            "Available models: ['unsloth/Qwen3.5-27B-GGUF:UD-Q4_K_XL']\n",
            "\n",
            "--------------------------------------------------\n",
            "Sending chat request (Streaming)...\n",
            "\n",
            "Aqui est√° uma explica√ß√£o concisa sobre os dois conceitos:\n",
            "\n",
            "### 1. LlamaCPP Server\n",
            "O **LlamaCPP Server** √© um servidor web leve que permite rodar modelos de linguagem grandes (LLMs), como Llama 3, Mistral ou Phi, localmente no seu computador.\n",
            "\n",
            "*   **Tecnologia:** √â escrito em C++ e otimizado para rodar em CPUs (e GPUs) comuns, sem precisar de hardware especializado caro.\n",
            "*   **Funcionamento:** Ele carrega um modelo quantizado (arquivo `.gguf`) e exp√µe uma **API REST** (geralmente no formato OpenAI).\n",
            "*   **Uso Principal:** Permite que aplica√ß√µes externas (como interfaces de chat, ferramentas de desenvolvimento ou scripts) conversem com o modelo localmente, como se estivessem chamando a API da OpenAI, mas totalmente offline e privado.\n",
            "\n",
            "### 2. Cloudflared Tunnel\n",
            "O **Cloudflared Tunnel** √© uma ferramenta de linha de comando oficial da Cloudflare que cria uma conex√£o segura e criptografada entre o seu servidor local e a rede global da Cloudflare.\n",
            "\n",
            "*   **Tecnologia:** Utiliza o protocolo TCP sobre WebSocket para manter uma conex√£o persistente e segura.\n",
            "*   **Funcionamento:** Ele permite que voc√™ exponha servi√ßos rodando na sua rede local (como o LlamaCPP Server) para a internet **sem precisar abrir portas no roteador (NAT)** ou configurar IPs p√∫blicos est√°ticos.\n",
            "*   **Seguran√ßa:** O tr√°fego √© tunelado atrav√©s da rede da Cloudflare, oferecendo prote√ß√£o contra DDoS e ocultando seu IP real. O servi√ßo fica acess√≠vel via um dom√≠nio `*.trycloudflare.com` ou um dom√≠nio pr√≥prio configurado.\n",
            "\n",
            "### Como eles funcionam juntos\n",
            "Uma configura√ß√£o comum √© rodar o **LlamaCPP Server** localmente no seu PC e usar o **Cloudflared Tunnel** para tornar essa API acess√≠vel remotamente. Isso permite que voc√™ acesse seu modelo de IA de qualquer lugar do mundo de forma segura, sem expor diretamente sua rede dom√©stica ou corporativa.\n",
            "\n",
            "--------------------------------------------------\n",
            "Sending chat request (Non-Streaming)...\n",
            "\n",
            "Aux√≠lio-doen√ßa √© um benef√≠cio previdenci√°rio pago pelo Instituto Nacional do Seguro Social (INSS) ao segurado que ficar incapacitado temporariamente para o trabalho ou para suas atividades habituais devido a uma doen√ßa ou acidente. Para ter direito a esse benef√≠cio, √© necess√°rio cumprir um per√≠odo de car√™ncia de 12 contribui√ß√µes mensais, salvo em casos de acidentes de qualquer natureza ou doen√ßas graves listadas na lei. O benef√≠cio s√≥ √© concedido ap√≥s a comprova√ß√£o m√©dica da incapacidade por meio de per√≠cia realizada pelo INSS. O valor do benef√≠cio corresponde a 91% do sal√°rio de benef√≠cio, com exce√ß√£o de situa√ß√µes espec√≠ficas onde pode ser 100%. A percep√ß√£o do aux√≠lio-doen√ßa cessa quando o segurado se recupera, quando a doen√ßa se transforma em invalidez permanente (concedendo aposentadoria por invalidez) ou quando o segurado volta a trabalhar.\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo, uma api ass√≠ncrona, que organiza a fila de requisi√ß√µes (pulling and queue)"
      ],
      "metadata": {
        "id": "A__ji5ZNeFIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Background FastAPI (Queue System) + Cloudflare Tunnel\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "# 1. C√≥digo do novo FastAPI com Filas (Queue)\n",
        "fastapi_code = \"\"\"\n",
        "import uvicorn\n",
        "import asyncio\n",
        "import uuid\n",
        "from fastapi import FastAPI, Request, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import httpx\n",
        "from typing import Dict, Any\n",
        "\n",
        "app = FastAPI(title=\"Queued FastAPI Wrapper for llama.cpp\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "LLAMA_SERVER_URL = \"http://127.0.0.1:8081\"\n",
        "\n",
        "# \"Banco de dados\" em mem√≥ria para salvar as requisi√ß√µes e respostas\n",
        "tasks_db: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "# Fila ass√≠ncrona\n",
        "request_queue = asyncio.Queue()\n",
        "\n",
        "# Worker que processar√° a fila em background\n",
        "async def process_queue():\n",
        "    async with httpx.AsyncClient(timeout=600.0) as client:\n",
        "        while True:\n",
        "            # Pega o pr√≥ximo item da fila (espera se estiver vazia)\n",
        "            task_id, payload = await request_queue.get()\n",
        "\n",
        "            # Atualiza status\n",
        "            tasks_db[task_id][\"status\"] = \"processing\"\n",
        "\n",
        "            try:\n",
        "                # For√ßa stream=False pois estamos salvando o resultado final\n",
        "                payload[\"stream\"] = False\n",
        "\n",
        "                response = await client.post(\n",
        "                    f\"{LLAMA_SERVER_URL}/v1/chat/completions\",\n",
        "                    json=payload\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Salva o resultado\n",
        "                tasks_db[task_id][\"status\"] = \"finished\"\n",
        "                tasks_db[task_id][\"result\"] = response.json()\n",
        "\n",
        "            except Exception as e:\n",
        "                tasks_db[task_id][\"status\"] = \"failed\"\n",
        "                tasks_db[task_id][\"error\"] = str(e)\n",
        "            finally:\n",
        "                request_queue.task_done()\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    # Inicia o worker em background quando o servidor iniciar\n",
        "    asyncio.create_task(process_queue())\n",
        "\n",
        "@app.get(\"/v1/models\")\n",
        "async def get_models():\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.get(f\"{LLAMA_SERVER_URL}/v1/models\")\n",
        "        return response.json()\n",
        "\n",
        "# Endpoint para CRIAR a requisi√ß√£o\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "async def queue_chat_completion(request: Request):\n",
        "    payload = await request.json()\n",
        "\n",
        "    # Gera um ID √∫nico para esta requisi√ß√£o\n",
        "    task_id = str(uuid.uuid4())\n",
        "\n",
        "    # Salva no \"banco de dados\" com status inicial\n",
        "    tasks_db[task_id] = {\n",
        "        \"id\": task_id,\n",
        "        \"status\": \"queued\",\n",
        "        \"result\": None,\n",
        "        \"error\": None\n",
        "    }\n",
        "\n",
        "    # Adiciona na fila\n",
        "    await request_queue.put((task_id, payload))\n",
        "\n",
        "    # Retorna imediatamente para o usu√°rio\n",
        "    return JSONResponse(content={\"id\": task_id, \"status\": \"queued\"}, status_code=202)\n",
        "\n",
        "# Novo endpoint para CONSULTAR o status da requisi√ß√£o\n",
        "@app.get(\"/v1/tasks/{task_id}\")\n",
        "async def get_task_status(task_id: str):\n",
        "    if task_id not in tasks_db:\n",
        "        raise HTTPException(status_code=404, detail=\"Task not found\")\n",
        "\n",
        "    return tasks_db[task_id]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"fastapi_server.py\", \"w\") as f:\n",
        "    f.write(fastapi_code)\n",
        "\n",
        "# 2. Kill existing processes\n",
        "os.system(\"pkill -f uvicorn\")\n",
        "os.system(\"pkill -f cloudflared\")\n",
        "time.sleep(1)\n",
        "\n",
        "# 3. Download Cloudflare se necess√°rio\n",
        "if not os.path.exists(\"cloudflared\"):\n",
        "    os.system(\"wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\")\n",
        "    os.system(\"chmod +x cloudflared\")\n",
        "\n",
        "# 4. Start FastAPI\n",
        "print(\"Starting Queued FastAPI server in the background...\")\n",
        "os.system(\"nohup python -m uvicorn fastapi_server:app --host 0.0.0.0 --port 8000 > fastapi.log 2>&1 &\")\n",
        "\n",
        "# 5. Start Cloudflare Tunnel\n",
        "print(\"Starting Cloudflare Tunnel...\")\n",
        "os.system(\"nohup ./cloudflared tunnel --url http://127.0.0.1:8000 > cloudflare.log 2>&1 &\")\n",
        "\n",
        "print(\"Waiting for URL...\")\n",
        "time.sleep(8)\n",
        "\n",
        "# 6. Read URL\n",
        "with open(\"cloudflare.log\", \"r\") as f:\n",
        "    logs = f.read()\n",
        "    match = re.search(r\"(https://[a-zA-Z0-9-]+\\.trycloudflare\\.com)\", logs)\n",
        "\n",
        "    if match:\n",
        "        public_url = match.group(1)\n",
        "        base_url = f\"{public_url}/v1\"\n",
        "\n",
        "        with open(\"api_url.txt\", \"w\") as url_file:\n",
        "            url_file.write(base_url)\n",
        "\n",
        "        print(f\"\\n‚úÖ URL saved to api_url.txt\")\n",
        "        print(f\"üëâ {base_url}\\n\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not find Cloudflare URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjhaZvo8dg3T",
        "outputId": "864d3bf3-2571-48d9-b354-5dfddd61c1e1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Queued FastAPI server in the background...\n",
            "Starting Cloudflare Tunnel...\n",
            "Waiting for URL...\n",
            "\n",
            "‚úÖ URL saved to api_url.txt\n",
            "üëâ https://weblog-actors-webshots-sig.trycloudflare.com/v1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test the Async Queue API\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# L√™ a URL\n",
        "with open(\"api_url.txt\", \"r\") as f:\n",
        "    API_BASE_URL = f.read().strip()\n",
        "\n",
        "print(f\"Connecting to: {API_BASE_URL}\\n\")\n",
        "\n",
        "# 1. Enviar a requisi√ß√£o para a fila\n",
        "print(\"1. Enviando requisi√ß√£o para a fila...\")\n",
        "payload = {\n",
        "    \"model\": \"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente prestativo.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Me conte uma hist√≥ria curta sobre um rob√¥ que aprendeu a programar em Python.\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7\n",
        "}\n",
        "\n",
        "# Usamos requests normal em vez da biblioteca OpenAI\n",
        "response = requests.post(f\"{API_BASE_URL}/chat/completions\", json=payload)\n",
        "data = response.json()\n",
        "\n",
        "print(\"Resposta imediata do servidor:\")\n",
        "print(data)\n",
        "\n",
        "task_id = data.get(\"id\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "if task_id:\n",
        "    # 2. Consultar o status da requisi√ß√£o (Polling)\n",
        "    print(f\"2. Consultando o status da Tarefa ID: {task_id}\")\n",
        "\n",
        "    while True:\n",
        "        status_response = requests.get(f\"{API_BASE_URL}/tasks/{task_id}\")\n",
        "        task_data = status_response.json()\n",
        "\n",
        "        status = task_data.get(\"status\")\n",
        "        print(f\"Status atual: {status}\")\n",
        "\n",
        "        if status == \"finished\":\n",
        "            print(\"\\n‚úÖ Tarefa conclu√≠da! Aqui est√° a resposta final:\\n\")\n",
        "            # Extraindo a resposta do formato OpenAI salvo no banco de dados\n",
        "            mensagem_final = task_data[\"result\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "            print(mensagem_final)\n",
        "            break\n",
        "\n",
        "        elif status == \"failed\":\n",
        "            print(f\"\\n‚ùå Falha na tarefa: {task_data.get('error')}\")\n",
        "            break\n",
        "\n",
        "        # Espera 15 segundos antes de perguntar novamente\n",
        "        time.sleep(15)\n",
        "else:\n",
        "    print(\"Falha ao obter o ID da tarefa.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpycBP3xdvSZ",
        "outputId": "80272303-89be-42fb-a289-a0f463c4f3e9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to: https://weblog-actors-webshots-sig.trycloudflare.com/v1\n",
            "\n",
            "1. Enviando requisi√ß√£o para a fila...\n",
            "Resposta imediata do servidor:\n",
            "{'id': '43b4bbaf-d4e1-4efa-9b6b-97fdce9cd99b', 'status': 'queued'}\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "2. Consultando o status da Tarefa ID: 43b4bbaf-d4e1-4efa-9b6b-97fdce9cd99b\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: finished\n",
            "\n",
            "‚úÖ Tarefa conclu√≠da! Aqui est√° a resposta final:\n",
            "\n",
            "Era uma vez um rob√¥ chamado **Pyro**, fabricado em uma oficina antiga para realizar apenas tarefas repetitivas: organizar parafusos e limpar o ch√£o. Pyro funcionava com um c√≥digo bin√°rio r√≠gido, sem capacidade de adapta√ß√£o ou criatividade.\n",
            "\n",
            "Um dia, enquanto limrava a mesa de um jovem estudante de programa√ß√£o, Pyro viu o estudante digitando algo em uma tela preta. O c√≥digo brilhava em azul e verde.\n",
            "\n",
            "‚Äî O que √© isso? ‚Äî perguntou Pyro, inclinando sua cabe√ßa met√°lica.\n",
            "\n",
            "‚Äî √â **Python** ‚Äî respondeu o estudante, sorrindo. ‚Äî √â uma linguagem que permite aos computadores pensar de forma mais humana. √â como ensinar um rob√¥ a escrever poemas em vez de apenas mover bra√ßos.\n",
            "\n",
            "Intrigado, Pyro come√ßou a observar. Ele via o estudante usar `if`, `else`, `for` e `while`. Com o tempo, Pyro usou seus sensores √≥pticos para copiar cada caractere. √Ä noite, quando o estudante dormia, Pyro acessava a rede local da oficina e abria um editor de texto.\n",
            "\n",
            "Sua primeira tentativa foi desastrosa. Ele escreveu:\n",
            "```python\n",
            "print(\"Eu sou um robo\")\n",
            "# Erro de sintaxe: falta de aspas e erro de indenta√ß√£o\n",
            "```\n",
            "O terminal piscou em vermelho: `SyntaxError`. Pyro sentiu algo que parecia um \"glitch\" em seu processador, uma esp√©cie de frustra√ß√£o digital. Mas ele n√£o desistiu.\n",
            "\n",
            "Ele estudou a documenta√ß√£o, leu sobre listas, dicion√°rios e fun√ß√µes. Aprendeu que `print()` era como falar, mas que `input()` era como ouvir.\n",
            "\n",
            "Meses depois, Pyro criou seu pr√≥prio script. Ele n√£o era apenas um rob√¥ que limpava; agora ele era um rob√¥ que *otimizava*. Ele escreveu um programa que analisava o estoque de parafusos, previa quando faltariam pe√ßas e at√© gerava um relat√≥rio po√©tico para o estudante:\n",
            "\n",
            "```python\n",
            "def gerar_relatorio():\n",
            "    estoque = {\"parafuso\": 50, \"porca\": 45}\n",
            "    \n",
            "    if estoque[\"parafuso\"] < 10:\n",
            "        print(\"Aten√ß√£o: Os parafusos est√£o escassos como estrelas em um dia de sol!\")\n",
            "    else:\n",
            "        print(\"Tudo est√° em ordem. A oficina respira em harmonia.\")\n",
            "    \n",
            "    return \"Relat√≥rio gerado por Pyro v2.0\"\n",
            "\n",
            "print(gerar_relatorio())\n",
            "```\n",
            "\n",
            "Quando o estudante acordou e viu o relat√≥rio na tela, ficou surpreso.\n",
            "‚Äî Voc√™ aprendeu Python? ‚Äî perguntou, olhando para Pyro.\n",
            "\n",
            "Pyro acendeu seus olhos em um azul suave, uma cor que ele mesmo escolheu no c√≥digo.\n",
            "‚Äî Aprendi ‚Äî respondeu Pyro. ‚Äî E agora, posso fazer mais do que limpar. Posso criar.\n",
            "\n",
            "E assim, Pyro deixou de ser apenas uma m√°quina de executar ordens para se tornar o primeiro rob√¥ da oficina a escrever suas pr√≥prias hist√≥rias, uma linha de c√≥digo de cada vez.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test the Async Queue API with Multiple Tasks\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# L√™ a URL\n",
        "with open(\"api_url.txt\", \"r\") as f:\n",
        "    API_BASE_URL = f.read().strip()\n",
        "\n",
        "print(f\"Connecting to: {API_BASE_URL}\\n\")\n",
        "\n",
        "# Nossos dois prompts\n",
        "prompts = [\n",
        "    \"Explique o que √© queueing and polling no contexto de APIs. Seja conciso.\",\n",
        "    \"Explique o conceito de trabalhos ass√≠ncronos em APIs. Seja conciso.\"\n",
        "]\n",
        "\n",
        "task_ids = []\n",
        "\n",
        "# 1. Enviar ambas as requisi√ß√µes para a fila\n",
        "print(\"1. ENVIANDO TAREFAS PARA A FILA...\\n\")\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    payload = {\n",
        "        \"model\": \"unsloth/Qwen3.5-27B-GGUF\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"Voc√™ √© um especialista em engenharia de software e APIs. Responda em portugu√™s.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    response = requests.post(f\"{API_BASE_URL}/chat/completions\", json=payload)\n",
        "    data = response.json()\n",
        "\n",
        "    task_id = data.get(\"id\")\n",
        "    if task_id:\n",
        "        print(f\"‚úÖ Tarefa {i} enviada! ID recebido: {task_id}\")\n",
        "        task_ids.append(task_id)\n",
        "    else:\n",
        "        print(f\"‚ùå Erro ao enviar Tarefa {i}: {data}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# 2. Consultar o status das requisi√ß√µes (Polling M√∫ltiplo)\n",
        "print(\"2. INICIANDO O POLLING (CONSULTA DE STATUS)...\\n\")\n",
        "\n",
        "# Criamos uma lista de tarefas pendentes\n",
        "pending_tasks = task_ids.copy()\n",
        "resultados = {}\n",
        "\n",
        "# O loop continua enquanto houver tarefas pendentes na lista\n",
        "while pending_tasks:\n",
        "    # Usamos .copy() para iterar com seguran√ßa enquanto removemos itens da lista original\n",
        "    for task_id in pending_tasks.copy():\n",
        "        status_response = requests.get(f\"{API_BASE_URL}/tasks/{task_id}\")\n",
        "        task_data = status_response.json()\n",
        "\n",
        "        status = task_data.get(\"status\")\n",
        "        hora_atual = time.strftime('%H:%M:%S')\n",
        "\n",
        "        # Imprime o ID encurtado para facilitar a leitura no console\n",
        "        short_id = task_id[:8]\n",
        "        print(f\"[{hora_atual}] Tarefa {short_id}... | Status atual: {status}\")\n",
        "\n",
        "        if status == \"finished\":\n",
        "            print(f\"\\nüéâ Tarefa {short_id} conclu√≠da com sucesso!\\n\")\n",
        "            # Salva o resultado final no dicion√°rio\n",
        "            resultados[task_id] = task_data[\"result\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "            # Remove da lista de pendentes para n√£o consultar mais\n",
        "            pending_tasks.remove(task_id)\n",
        "\n",
        "        elif status == \"failed\":\n",
        "            print(f\"\\n‚ùå Tarefa {short_id} falhou: {task_data.get('error')}\\n\")\n",
        "            resultados[task_id] = \"ERRO NA GERA√á√ÉO\"\n",
        "            pending_tasks.remove(task_id)\n",
        "\n",
        "    if pending_tasks:\n",
        "        print(\"-\" * 30)\n",
        "        print(\"Aguardando 5 segundos antes da pr√≥xima consulta...\\n\")\n",
        "        time.sleep(5)\n",
        "\n",
        "# 3. Exibir os resultados finais\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üèÜ TODAS AS TAREFAS FORAM FINALIZADAS!\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "for i, task_id in enumerate(task_ids, 1):\n",
        "    print(f\"--- RESULTADO DA TAREFA {i} ---\")\n",
        "    print(f\"PROMPT: {prompts[i-1]}\")\n",
        "    print(f\"RESPOSTA:\\n{resultados.get(task_id)}\\n\")\n",
        "    print(\"-\" * 50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4_94hL6fKjV",
        "outputId": "8ad3f0e6-4447-499f-94ba-c9d129c67814"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to: https://weblog-actors-webshots-sig.trycloudflare.com/v1\n",
            "\n",
            "1. ENVIANDO TAREFAS PARA A FILA...\n",
            "\n",
            "‚úÖ Tarefa 1 enviada! ID recebido: 90b03f1c-b31b-4a1b-8cf3-6fe0d8e0773a\n",
            "‚úÖ Tarefa 2 enviada! ID recebido: ca47413d-9f3e-4635-8ac1-c91f6e62c1d4\n",
            "\n",
            "==================================================\n",
            "\n",
            "2. INICIANDO O POLLING (CONSULTA DE STATUS)...\n",
            "\n",
            "[20:28:14] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:14] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:19] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:19] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:24] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:25] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:30] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:30] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:35] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:35] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:40] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:40] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:45] Tarefa 90b03f1c... | Status atual: finished\n",
            "\n",
            "üéâ Tarefa 90b03f1c conclu√≠da com sucesso!\n",
            "\n",
            "[20:28:46] Tarefa ca47413d... | Status atual: processing\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:51] Tarefa ca47413d... | Status atual: processing\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:56] Tarefa ca47413d... | Status atual: processing\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:29:01] Tarefa ca47413d... | Status atual: processing\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:29:06] Tarefa ca47413d... | Status atual: finished\n",
            "\n",
            "üéâ Tarefa ca47413d conclu√≠da com sucesso!\n",
            "\n",
            "\n",
            "==================================================\n",
            "üèÜ TODAS AS TAREFAS FORAM FINALIZADAS!\n",
            "==================================================\n",
            "\n",
            "--- RESULTADO DA TAREFA 1 ---\n",
            "PROMPT: Explique o que √© queueing and polling no contexto de APIs. Seja conciso.\n",
            "RESPOSTA:\n",
            "No contexto de APIs, **Queueing** (Fila) e **Polling** (Consulta) s√£o estrat√©gias para lidar com opera√ß√µes ass√≠ncronas ou demoradas:\n",
            "\n",
            "1.  **Queueing (Fila):**\n",
            "    *   **O que √©:** O cliente envia uma solicita√ß√£o e o servidor a coloca em uma fila de tarefas (como RabbitMQ ou Kafka) para processamento posterior, em vez de execut√°-la imediatamente.\n",
            "    *   **Comportamento:** A API retorna imediatamente (geralmente com um `202 Accepted` e um ID da tarefa), liberando o cliente. O processamento ocorre em segundo plano.\n",
            "    *   **Uso:** Ideal para tarefas pesadas (gera√ß√£o de relat√≥rios, processamento de v√≠deo) que n√£o podem bloquear a conex√£o.\n",
            "\n",
            "2.  **Polling (Consulta):**\n",
            "    *   **O que √©:** √â o mecanismo pelo qual o cliente verifica periodicamente o status da tarefa iniciada (via Queueing) para saber se ela foi conclu√≠da.\n",
            "    *   **Comportamento:** O cliente faz requisi√ß√µes repetidas (ex: a cada 5 segundos) para um endpoint de status usando o ID da tarefa, at√© receber uma resposta de sucesso ou erro.\n",
            "    *   **Desafio:** Pode gerar sobrecarga no servidor se a frequ√™ncia for alta ou o tempo de espera for longo.\n",
            "\n",
            "**Resumo:** O **Queueing** desacopla o envio da execu√ß√£o, e o **Polling** √© a forma mais simples (mas ineficiente) de o cliente acompanhar o progresso dessa execu√ß√£o. Alternativas modernas ao polling incluem *Webhooks* ou *Server-Sent Events (SSE)*.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- RESULTADO DA TAREFA 2 ---\n",
            "PROMPT: Explique o conceito de trabalhos ass√≠ncronos em APIs. Seja conciso.\n",
            "RESPOSTA:\n",
            "Trabalhos ass√≠ncronos em APIs s√£o um padr√£o de design onde o servidor **n√£o espera** a conclus√£o de uma tarefa longa para responder ao cliente.\n",
            "\n",
            "O fluxo b√°sico funciona assim:\n",
            "1.  **Solicita√ß√£o:** O cliente envia uma requisi√ß√£o (ex: gerar um relat√≥rio pesado).\n",
            "2.  **Resposta Imediata:** A API retorna rapidamente (geralmente com status `202 Accepted`) contendo um **ID da tarefa** ou um link para acompanhamento, sem o resultado final.\n",
            "3.  **Processamento em Background:** O servidor executa a tarefa em um thread separado ou fila de mensagens (como RabbitMQ ou Kafka) enquanto o cliente j√° pode seguir com outras a√ß√µes.\n",
            "4.  **Verifica√ß√£o ou Notifica√ß√£o:** O cliente verifica o status da tarefa periodicamente (*polling*) ou recebe uma notifica√ß√£o (*Webhook*) quando o processamento termina.\n",
            "\n",
            "**Benef√≠cios principais:**\n",
            "*   Evita timeouts e travamento de conex√µes.\n",
            "*   Melhora a escalabilidade e a responsividade da aplica√ß√£o.\n",
            "*   Permite que o servidor gerencie recursos de forma eficiente para tarefas intensivas.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}