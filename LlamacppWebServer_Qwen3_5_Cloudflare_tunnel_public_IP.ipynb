{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO8CsBqhUmUgaRpF/g616aF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiopauli/Qwen3.5-colab/blob/main/LlamacppWebServer_Qwen3_5_Cloudflare_tunnel_public_IP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kKec3km0unwH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get update -qq\n",
        "!apt-get install -y build-essential cmake git -qq\n",
        "!pip install -q huggingface_hub[cli]\n",
        "\n",
        "# Build latest llama.cpp with full CUDA support for L4\n",
        "!git clone --depth 1 https://github.com/ggml-org/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "!cmake -B build -DGGML_CUDA=ON -DCMAKE_BUILD_TYPE=Release\n",
        "!cmake --build build --config Release -j4\n",
        "\n",
        "print(\"âœ… Build finished! Ready for Cell 2.\")\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import subprocess, time, threading, os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Set your custom port here\n",
        "PORT = 8081  # Change this if needed\n",
        "\n",
        "# Unsloth recommended cache (forces clean download to correct folder)\n",
        "os.environ[\"LLAMA_CACHE\"] = \"unsloth/Qwen3.5-27B-GGUF\"\n",
        "\n",
        "print(\"ðŸ“¥ First run will auto-download UD-Q4_K_XL.gguf (~16.7 GB from Unsloth)...\")\n",
        "\n",
        "# === Install lsof if needed & Kill any process on the port ===\n",
        "!apt-get install -y lsof  # Usually already installed, but to be safe\n",
        "print(f\"ðŸ›‘ Killing any process on port {PORT}...\")\n",
        "!kill $(lsof -t -i:{PORT}) 2>/dev/null || true\n",
        "\n",
        "# === Cloudflared Tunnel ===\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb 2>/dev/null || true\n",
        "\n",
        "def start_tunnel():\n",
        "    tunnel_process = subprocess.Popen(\n",
        "        [\"cloudflared\", \"tunnel\", \"--url\", f\"http://127.0.0.1:{PORT}\", \"--no-autoupdate\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    for line in iter(tunnel_process.stdout.readline, ''):\n",
        "        print(line.strip())\n",
        "    tunnel_process.stdout.close()\n",
        "    tunnel_process.wait()\n",
        "\n",
        "print(\"ðŸŒ Starting free public Cloudflare Tunnel...\")\n",
        "tunnel_thread = threading.Thread(target=start_tunnel, daemon=True)\n",
        "tunnel_thread.start()\n",
        "time.sleep(8)\n",
        "\n",
        "# === llama-server + WebUI with YOUR exact settings ===\n",
        "print(\"\\nðŸš€ Starting llama.cpp server + built-in WebUI\")\n",
        "print(\"   Model : unsloth/Qwen3.5-27B UD-Q4_K_XL (16.7 GB)\")\n",
        "print(\"   Context: 16384 | Thinking: DISABLED | Flash Attention ON\")\n",
        "print(\"   Temp 0.7 | top-p 0.8 | top-k 20 | min-p 0.00\")\n",
        "print(\"   Wait 45â€“120 seconds while model loads on L4...\\n\")\n",
        "print(\"â•\"*85)\n",
        "print(f\"âœ… YOUR PUBLIC WEBUI LINK will appear below (something.trycloudflare.com on port {PORT})\")\n",
        "print(\"   Open it in any browser â†’ full ChatGPT-style UI\")\n",
        "print(\"â•\"*85)\n",
        "\n",
        "server_cmd = f\"\"\"\n",
        "/content/llama.cpp/build/bin/llama-server \\\n",
        "  -hf unsloth/Qwen3.5-27B-GGUF:UD-Q4_K_XL \\\n",
        "  --jinja \\\n",
        "  -c 16384 \\\n",
        "  --n-gpu-layers 99 \\\n",
        "  --temp 0.7 \\\n",
        "  --top-p 0.8 \\\n",
        "  --top-k 20 \\\n",
        "  --min-p 0.00 \\\n",
        "  --chat-template-kwargs '{{\"enable_thinking\": false}}' \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port {PORT} \\\n",
        "  --alias \"Qwen3.5-27B-Unsloth-UD-Q4\"\n",
        "\"\"\"\n",
        "\n",
        "!{server_cmd}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWEenR4n3mrQ",
        "outputId": "f44cc521-558d-4d17-e4af-3c659c8aaf81"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¥ First run will auto-download UD-Q4_K_XL.gguf (~16.7 GB from Unsloth)...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "lsof is already the newest version (4.93.2+dfsg-1.1build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 86 not upgraded.\n",
            "ðŸ›‘ Killing any process on port 8081...\n",
            "(Reading database ... 121856 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2026.2.0) over (2026.2.0) ...\n",
            "Setting up cloudflared (2026.2.0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "ðŸŒ Starting free public Cloudflare Tunnel...\n",
            "2026-02-26T12:19:31Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2026-02-26T12:19:31Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2026-02-26T12:19:35Z INF +--------------------------------------------------------------------------------------------+\n",
            "2026-02-26T12:19:35Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2026-02-26T12:19:35Z INF |  https://pilot-society-reprint-completion.trycloudflare.com                                |\n",
            "2026-02-26T12:19:35Z INF +--------------------------------------------------------------------------------------------+\n",
            "2026-02-26T12:19:35Z INF Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "2026-02-26T12:19:35Z INF Version 2026.2.0 (Checksum 176746db3be7dc7bd48f3dd287c8930a4645ebb6e6700f883fddda5a4c307c16)\n",
            "2026-02-26T12:19:35Z INF GOOS: linux, GOVersion: go1.24.13, GoArch: amd64\n",
            "2026-02-26T12:19:35Z INF Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://127.0.0.1:8081]\n",
            "2026-02-26T12:19:35Z INF cloudflared will not automatically update if installed by a package manager.\n",
            "2026-02-26T12:19:35Z INF Generated Connector ID: c0b68695-4248-4738-b9d2-8e225bbc05e6\n",
            "2026-02-26T12:19:35Z INF Initial protocol quic\n",
            "2026-02-26T12:19:35Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2026-02-26T12:19:35Z INF ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "2026-02-26T12:19:35Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2026-02-26T12:19:35Z INF ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "2026-02-26T12:19:35Z INF Starting metrics server on 127.0.0.1:20242/metrics\n",
            "2026-02-26T12:19:35Z INF Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] connIndex=0 event=0 ip=198.41.192.107\n",
            "2026/02/26 12:19:35 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "2026-02-26T12:19:35Z INF Registered tunnel connection connIndex=0 connection=1943113b-5860-4698-911b-8a0bab58193e event=0 ip=198.41.192.107 location=lax10 protocol=quic\n",
            "\n",
            "ðŸš€ Starting llama.cpp server + built-in WebUI\n",
            "   Model : unsloth/Qwen3.5-27B UD-Q4_K_XL (16.7 GB)\n",
            "   Context: 16384 | Thinking: DISABLED | Flash Attention ON\n",
            "   Temp 0.7 | top-p 0.8 | top-k 20 | min-p 0.00\n",
            "   Wait 45â€“120 seconds while model loads on L4...\n",
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "âœ… YOUR PUBLIC WEBUI LINK will appear below (something.trycloudflare.com on port 8081)\n",
            "   Open it in any browser â†’ full ChatGPT-style UI\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n",
            "common_download_file_single_online: no previous model file found unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_preset.ini\n",
            "common_download_file_single_online: HEAD failed, status: 404\n",
            "\u001b[0mno remote preset found, skipping\n",
            "common_download_file_single_online: using cached file (same etag): unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_Qwen3.5-27B-UD-Q4_K_XL.gguf\n",
            "common_download_file_single_online: using cached file (same etag): unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_mmproj-BF16.gguf\n",
            "main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true\n",
            "build: 1 (efba35a) with GNU 11.4.0 for Linux x86_64\n",
            "system info: n_threads = 6, n_threads_batch = 6, total_threads = 12\n",
            "\n",
            "system_info: n_threads = 6 (n_threads_batch = 6) / 12 | CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "Running without SSL\n",
            "init: using 11 threads for HTTP server\n",
            "start: binding port with default address family\n",
            "main: loading model\n",
            "srv    load_model: loading model 'unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_Qwen3.5-27B-UD-Q4_K_XL.gguf'\n",
            "common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on\n",
            "llama_params_fit_impl: projected to use 17379 MiB of device memory vs. 21769 MiB of free device memory\n",
            "llama_params_fit_impl: will leave 4390 >= 1024 MiB of free device memory, no changes needed\n",
            "llama_params_fit: successfully fit params to free device memory\n",
            "llama_params_fit: fitting params to free memory took 0.84 seconds\n",
            "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L4) (0000:00:03.0) - 22369 MiB free\n",
            "llama_model_loader: loaded meta data with 42 key-value pairs and 851 tensors from unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_Qwen3.5-27B-UD-Q4_K_XL.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen35\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20\n",
            "llama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.950000\n",
            "llama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.600000\n",
            "llama_model_loader: - kv   5:                               general.name str              = Qwen3.5-27B\n",
            "llama_model_loader: - kv   6:                           general.basename str              = Qwen3.5-27B\n",
            "llama_model_loader: - kv   7:                       general.quantized_by str              = Unsloth\n",
            "llama_model_loader: - kv   8:                         general.size_label str              = 27B\n",
            "llama_model_loader: - kv   9:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv  10:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3.5-2...\n",
            "llama_model_loader: - kv  11:                           general.repo_url str              = https://huggingface.co/unsloth\n",
            "llama_model_loader: - kv  12:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
            "llama_model_loader: - kv  13:                         qwen35.block_count u32              = 64\n",
            "llama_model_loader: - kv  14:                      qwen35.context_length u32              = 262144\n",
            "llama_model_loader: - kv  15:                    qwen35.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv  16:                 qwen35.feed_forward_length u32              = 17408\n",
            "llama_model_loader: - kv  17:                qwen35.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  18:             qwen35.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  19:             qwen35.rope.dimension_sections arr[i32,4]       = [11, 11, 10, 0]\n",
            "llama_model_loader: - kv  20:                      qwen35.rope.freq_base f32              = 10000000.000000\n",
            "llama_model_loader: - kv  21:    qwen35.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  22:                qwen35.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  23:              qwen35.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  24:                     qwen35.ssm.conv_kernel u32              = 4\n",
            "llama_model_loader: - kv  25:                      qwen35.ssm.state_size u32              = 128\n",
            "llama_model_loader: - kv  26:                     qwen35.ssm.group_count u32              = 16\n",
            "llama_model_loader: - kv  27:                  qwen35.ssm.time_step_rank u32              = 48\n",
            "llama_model_loader: - kv  28:                      qwen35.ssm.inner_size u32              = 6144\n",
            "llama_model_loader: - kv  29:             qwen35.full_attention_interval u32              = 4\n",
            "llama_model_loader: - kv  30:                qwen35.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  31:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  32:                         tokenizer.ggml.pre str              = qwen35\n",
            "llama_model_loader: - kv  33:                      tokenizer.ggml.tokens arr[str,248320]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  34:                  tokenizer.ggml.token_type arr[i32,248320]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  35:                      tokenizer.ggml.merges arr[str,247587]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
            "llama_model_loader: - kv  36:                tokenizer.ggml.eos_token_id u32              = 248046\n",
            "llama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 248044\n",
            "llama_model_loader: - kv  38:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  39:                    tokenizer.chat_template str              = {%- set image_count = namespace(value...\n",
            "llama_model_loader: - kv  40:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  41:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:  353 tensors\n",
            "llama_model_loader: - type q4_K:  359 tensors\n",
            "llama_model_loader: - type q5_K:   96 tensors\n",
            "llama_model_loader: - type q6_K:   43 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 15.57 GiB (4.97 BPW) \n",
            "load: 0 unused tokens\n",
            "load: printing all EOG tokens:\n",
            "load:   - 248044 ('<|endoftext|>')\n",
            "load:   - 248046 ('<|im_end|>')\n",
            "load:   - 248063 ('<|fim_pad|>')\n",
            "load:   - 248064 ('<|repo_name|>')\n",
            "load:   - 248065 ('<|file_sep|>')\n",
            "load: special tokens cache size = 33\n",
            "load: token to piece cache size = 1.7581 MB\n",
            "print_info: arch                  = qwen35\n",
            "print_info: vocab_only            = 0\n",
            "print_info: no_alloc              = 0\n",
            "print_info: n_ctx_train           = 262144\n",
            "print_info: n_embd                = 5120\n",
            "print_info: n_embd_inp            = 5120\n",
            "print_info: n_layer               = 64\n",
            "print_info: n_head                = 24\n",
            "print_info: n_head_kv             = 4\n",
            "print_info: n_rot                 = 64\n",
            "print_info: n_swa                 = 0\n",
            "print_info: is_swa_any            = 0\n",
            "print_info: n_embd_head_k         = 256\n",
            "print_info: n_embd_head_v         = 256\n",
            "print_info: n_gqa                 = 6\n",
            "print_info: n_embd_k_gqa          = 1024\n",
            "print_info: n_embd_v_gqa          = 1024\n",
            "print_info: f_norm_eps            = 0.0e+00\n",
            "print_info: f_norm_rms_eps        = 1.0e-06\n",
            "print_info: f_clamp_kqv           = 0.0e+00\n",
            "print_info: f_max_alibi_bias      = 0.0e+00\n",
            "print_info: f_logit_scale         = 0.0e+00\n",
            "print_info: f_attn_scale          = 0.0e+00\n",
            "print_info: n_ff                  = 17408\n",
            "print_info: n_expert              = 0\n",
            "print_info: n_expert_used         = 0\n",
            "print_info: n_expert_groups       = 0\n",
            "print_info: n_group_used          = 0\n",
            "print_info: causal attn           = 1\n",
            "print_info: pooling type          = 0\n",
            "print_info: rope type             = 40\n",
            "print_info: rope scaling          = linear\n",
            "print_info: freq_base_train       = 10000000.0\n",
            "print_info: freq_scale_train      = 1\n",
            "print_info: n_ctx_orig_yarn       = 262144\n",
            "print_info: rope_yarn_log_mul     = 0.0000\n",
            "print_info: rope_finetuned        = unknown\n",
            "print_info: mrope sections        = [11, 11, 10, 0]\n",
            "print_info: ssm_d_conv            = 4\n",
            "print_info: ssm_d_inner           = 6144\n",
            "print_info: ssm_d_state           = 128\n",
            "print_info: ssm_dt_rank           = 48\n",
            "print_info: ssm_n_group           = 16\n",
            "print_info: ssm_dt_b_c_rms        = 0\n",
            "print_info: model type            = ?B\n",
            "print_info: model params          = 26.90 B\n",
            "print_info: general.name          = Qwen3.5-27B\n",
            "print_info: vocab type            = BPE\n",
            "print_info: n_vocab               = 248320\n",
            "print_info: n_merges              = 247587\n",
            "print_info: BOS token             = 11 ','\n",
            "print_info: EOS token             = 248046 '<|im_end|>'\n",
            "print_info: EOT token             = 248046 '<|im_end|>'\n",
            "print_info: PAD token             = 248044 '<|endoftext|>'\n",
            "print_info: LF token              = 198 'ÄŠ'\n",
            "print_info: FIM PRE token         = 248060 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token         = 248062 '<|fim_suffix|>'\n",
            "print_info: FIM MID token         = 248061 '<|fim_middle|>'\n",
            "print_info: FIM PAD token         = 248063 '<|fim_pad|>'\n",
            "print_info: FIM REP token         = 248064 '<|repo_name|>'\n",
            "print_info: FIM SEP token         = 248065 '<|file_sep|>'\n",
            "print_info: EOG token             = 248044 '<|endoftext|>'\n",
            "print_info: EOG token             = 248046 '<|im_end|>'\n",
            "print_info: EOG token             = 248063 '<|fim_pad|>'\n",
            "print_info: EOG token             = 248064 '<|repo_name|>'\n",
            "print_info: EOG token             = 248065 '<|file_sep|>'\n",
            "print_info: max token length      = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true, direct_io = false)\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloading 63 repeating layers to GPU\n",
            "load_tensors: offloaded 65/65 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =   682.03 MiB\n",
            "load_tensors:        CUDA0 model buffer size = 15261.52 MiB\n",
            "............................................................................................\n",
            "common_init_result: added <|endoftext|> logit bias = -inf\n",
            "common_init_result: added <|im_end|> logit bias = -inf\n",
            "common_init_result: added <|fim_pad|> logit bias = -inf\n",
            "common_init_result: added <|repo_name|> logit bias = -inf\n",
            "common_init_result: added <|file_sep|> logit bias = -inf\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 4\n",
            "llama_context: n_ctx         = 16384\n",
            "llama_context: n_ctx_seq     = 16384\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = auto\n",
            "llama_context: kv_unified    = true\n",
            "llama_context: freq_base     = 10000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_seq (16384) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\n",
            "\u001b[0mllama_context:  CUDA_Host  output buffer size =     3.79 MiB\n",
            "llama_kv_cache:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_kv_cache: size = 1024.00 MiB ( 16384 cells,  16 layers,  4/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_memory_recurrent:      CUDA0 RS buffer size =   598.50 MiB\n",
            "llama_memory_recurrent: size =  598.50 MiB (     4 cells,  64 layers,  4 seqs), R (f32):   22.50 MiB, S (f32):  576.00 MiB\n",
            "sched_reserve: reserving ...\n",
            "sched_reserve: Flash Attention was auto, set to enabled\n",
            "sched_reserve:      CUDA0 compute buffer size =   495.00 MiB\n",
            "sched_reserve:  CUDA_Host compute buffer size =    52.02 MiB\n",
            "sched_reserve: graph nodes  = 8409 (with bs=512), 4713 (with bs=1)\n",
            "sched_reserve: graph splits = 2\n",
            "sched_reserve: reserve took 58.34 ms, sched copies = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "\u001b[0mclip_model_loader: model name:   Qwen3.5-27B\n",
            "clip_model_loader: description:  \n",
            "clip_model_loader: GGUF version: 3\n",
            "clip_model_loader: alignment:    32\n",
            "clip_model_loader: n_tensors:    334\n",
            "clip_model_loader: n_kv:         31\n",
            "\n",
            "clip_model_loader: has vision encoder\n",
            "clip_ctx: CLIP using CUDA0 backend\n",
            "load_hparams: Qwen-VL models require at minimum 1024 image tokens to function correctly on grounding tasks\n",
            "\u001b[0mload_hparams: if you encounter problems with accuracy, try adding --image-min-tokens 1024\n",
            "\u001b[0mload_hparams: more info: https://github.com/ggml-org/llama.cpp/issues/16842\n",
            "\n",
            "\u001b[0mload_hparams: projector:          qwen3vl_merger\n",
            "load_hparams: n_embd:             1152\n",
            "load_hparams: n_head:             16\n",
            "load_hparams: n_ff:               4304\n",
            "load_hparams: n_layer:            27\n",
            "load_hparams: ffn_op:             gelu\n",
            "load_hparams: projection_dim:     5120\n",
            "\n",
            "--- vision hparams ---\n",
            "load_hparams: image_size:         768\n",
            "load_hparams: patch_size:         16\n",
            "load_hparams: has_llava_proj:     0\n",
            "load_hparams: minicpmv_version:   0\n",
            "load_hparams: n_merge:            2\n",
            "load_hparams: n_wa_pattern: 0\n",
            "load_hparams: image_min_pixels:   8192\n",
            "load_hparams: image_max_pixels:   4194304\n",
            "\n",
            "load_hparams: model size:         887.99 MiB\n",
            "load_hparams: metadata size:      0.12 MiB\n",
            "warmup: warmup with image size = 1472 x 1472\n",
            "alloc_compute_meta:      CUDA0 compute buffer size =   248.10 MiB\n",
            "alloc_compute_meta:        CPU compute buffer size =    24.93 MiB\n",
            "alloc_compute_meta: graph splits = 1, nodes = 823\n",
            "warmup: flash attention is enabled\n",
            "srv    load_model: loaded multimodal model, 'unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_mmproj-BF16.gguf'\n",
            "srv    load_model: initializing slots, n_slots = 4\n",
            "common_speculative_is_compat: the target context does not support partial sequence removal\n",
            "\u001b[0msrv    load_model: speculative decoding not supported by this context\n",
            "\u001b[0mslot   load_model: id  0 | task -1 | new slot, n_ctx = 16384\n",
            "slot   load_model: id  1 | task -1 | new slot, n_ctx = 16384\n",
            "slot   load_model: id  2 | task -1 | new slot, n_ctx = 16384\n",
            "slot   load_model: id  3 | task -1 | new slot, n_ctx = 16384\n",
            "srv    load_model: prompt cache is enabled, size limit: 8192 MiB\n",
            "\u001b[0msrv    load_model: use `--cache-ram 0` to disable the prompt cache\n",
            "\u001b[0msrv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391\n",
            "\u001b[0minit: chat template, example_format: '<|im_start|>system\n",
            "You are a helpful assistant<|im_end|>\n",
            "<|im_start|>user\n",
            "Hello<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Hi there<|im_end|>\n",
            "<|im_start|>user\n",
            "How are you?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "'\n",
            "srv          init: init: chat template, thinking = 1\n",
            "main: model loaded\n",
            "main: server is listening on http://0.0.0.0:8081\n",
            "main: starting the main loop...\n",
            "srv  update_slots: all slots are idle\n",
            "srv  log_server_r: done request: GET / 127.0.0.1 200\n",
            "srv  params_from_: Chat format: peg-constructed\n",
            "slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1\n",
            "slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> ?min-p -> ?xtc -> temp-ext -> dist \n",
            "slot launch_slot_: id  3 | task 0 | processing task, is_child = 0\n",
            "slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 16384, n_keep = 0, task.n_tokens = 5185\n",
            "slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)\n",
            "srv  log_server_r: done request: POST /v1/chat/completions 127.0.0.1 200\n",
            "slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.394986\n",
            "slot update_slots: id  3 | task 0 | n_tokens = 2048, memory_seq_rm [2048, end)\n",
            "slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.789971\n",
            "slot update_slots: id  3 | task 0 | n_tokens = 4096, memory_seq_rm [4096, end)\n",
            "slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 4673, batch.n_tokens = 577, progress = 0.901254\n",
            "slot update_slots: id  3 | task 0 | n_tokens = 4673, memory_seq_rm [4673, end)\n",
            "slot init_sampler: id  3 | task 0 | init sampler, took 0.86 ms, tokens: text = 5185, total = 5185\n",
            "slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 4672, pos_max = 4672, n_tokens = 4673, size = 149.626 MiB)\n",
            "\u001b[0mslot update_slots: id  3 | task 0 | prompt processing done, n_tokens = 5185, batch.n_tokens = 512\n",
            "slot print_timing: id  3 | task 0 | \n",
            "prompt eval time =    8083.20 ms /  5185 tokens (    1.56 ms per token,   641.45 tokens per second)\n",
            "       eval time =   49349.32 ms /   622 tokens (   79.34 ms per token,    12.60 tokens per second)\n",
            "      total time =   57432.52 ms /  5807 tokens\n",
            "slot      release: id  3 | task 0 | stop processing: n_tokens = 5806, truncated = 0\n",
            "srv  update_slots: all slots are idle\n",
            "srv  params_from_: Chat format: peg-constructed\n",
            "slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.888 (> 0.100 thold), f_keep = 0.892\n",
            "slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> ?min-p -> ?xtc -> temp-ext -> dist \n",
            "slot launch_slot_: id  3 | task 626 | processing task, is_child = 0\n",
            "slot update_slots: id  3 | task 626 | new prompt, n_ctx_slot = 16384, n_keep = 0, task.n_tokens = 5837\n",
            "slot update_slots: id  3 | task 626 | n_past = 5181, slot.prompt.tokens.size() = 5806, seq_id = 3, pos_min = 5805, n_swa = 1\n",
            "\u001b[0mslot update_slots: id  3 | task 626 | restored context checkpoint (pos_min = 4672, pos_max = 4672, n_tokens = 4673, size = 149.626 MiB)\n",
            "\u001b[0mslot update_slots: id  3 | task 626 | n_tokens = 4674, memory_seq_rm [4674, end)\n",
            "srv  log_server_r: done request: POST /v1/chat/completions 127.0.0.1 200\n",
            "slot update_slots: id  3 | task 626 | prompt processing progress, n_tokens = 5325, batch.n_tokens = 651, progress = 0.912284\n",
            "find_slot: non-consecutive token position 5185 after 4672 for sequence 3 with 512 new tokens\n",
            "\u001b[0mfind_slot: non-consecutive token position 5185 after 4672 for sequence 3 with 512 new tokens\n",
            "\u001b[0mslot update_slots: id  3 | task 626 | n_tokens = 5325, memory_seq_rm [5325, end)\n",
            "slot init_sampler: id  3 | task 626 | init sampler, took 0.97 ms, tokens: text = 5837, total = 5837\n",
            "slot update_slots: id  3 | task 626 | created context checkpoint 2 of 8 (pos_min = 5324, pos_max = 5324, n_tokens = 5325, size = 149.626 MiB)\n",
            "\u001b[0mslot update_slots: id  3 | task 626 | prompt processing done, n_tokens = 5837, batch.n_tokens = 512\n",
            "slot print_timing: id  3 | task 626 | \n",
            "prompt eval time =    2026.14 ms /  1163 tokens (    1.74 ms per token,   574.00 tokens per second)\n",
            "       eval time =   29700.96 ms /   372 tokens (   79.84 ms per token,    12.52 tokens per second)\n",
            "      total time =   31727.10 ms /  1535 tokens\n",
            "slot      release: id  3 | task 626 | stop processing: n_tokens = 6208, truncated = 0\n",
            "srv  update_slots: all slots are idle\n",
            "srv  params_from_: Chat format: peg-constructed\n",
            "slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1\n",
            "slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> ?min-p -> ?xtc -> temp-ext -> dist \n",
            "slot launch_slot_: id  2 | task 1000 | processing task, is_child = 0\n",
            "slot update_slots: id  2 | task 1000 | new prompt, n_ctx_slot = 16384, n_keep = 0, task.n_tokens = 287\n",
            "slot update_slots: id  2 | task 1000 | n_tokens = 0, memory_seq_rm [0, end)\n",
            "srv  log_server_r: done request: POST /v1/chat/completions 127.0.0.1 200\n",
            "slot update_slots: id  2 | task 1000 | prompt processing progress, n_tokens = 11, batch.n_tokens = 11, progress = 0.038328\n",
            "slot update_slots: id  2 | task 1000 | n_tokens = 11, memory_seq_rm [11, end)\n",
            "srv  process_chun: processing image...\n",
            "encoding image slice...\n",
            "image slice encoded in 285 ms\n",
            "decoding image batch 1/1, n_tokens_batch = 266\n",
            "find_slot: non-consecutive token position 11 after 10 for sequence 2 with 266 new tokens\n",
            "\u001b[0mfind_slot: non-consecutive token position 11 after 10 for sequence 2 with 266 new tokens\n",
            "\u001b[0mimage decoded (batch 1/1) in 431 ms\n",
            "srv  process_chun: image processed in 716 ms\n",
            "slot init_sampler: id  2 | task 1000 | init sampler, took 0.01 ms, tokens: text = 21, total = 287\n",
            "slot update_slots: id  2 | task 1000 | prompt processing done, n_tokens = 287, batch.n_tokens = 10\n",
            "find_slot: non-consecutive token position 58 after 11 for sequence 2 with 10 new tokens\n",
            "\u001b[0mfind_slot: non-consecutive token position 58 after 11 for sequence 2 with 10 new tokens\n",
            "\u001b[0mslot print_timing: id  2 | task 1000 | \n",
            "prompt eval time =    1003.80 ms /   287 tokens (    3.50 ms per token,   285.91 tokens per second)\n",
            "       eval time =  107835.19 ms /  1339 tokens (   80.53 ms per token,    12.42 tokens per second)\n",
            "      total time =  108839.00 ms /  1626 tokens\n",
            "slot      release: id  2 | task 1000 | stop processing: n_tokens = 1625, truncated = 0\n",
            "srv  update_slots: all slots are idle\n",
            "Received second interrupt, terminating immediately.\n",
            "2026-02-26T12:27:28Z INF Initiating graceful shutdown due to signal interrupt ...\n",
            "2026-02-26T12:27:28Z INF Initiating graceful shutdown due to signal interrupt ...\n",
            "2026-02-26T12:27:28Z ERR failed to run the datagram handler error=\"Application error 0x0 (remote)\" connIndex=0 event=0 ip=198.41.192.107\n",
            "2026-02-26T12:27:28Z ERR failed to serve tunnel connection error=\"accept stream listener encountered a failure while serving\" connIndex=0 event=0 ip=198.41.192.107\n",
            "2026-02-26T12:27:28Z ERR Serve tunnel error error=\"accept stream listener encountered a failure while serving\" connIndex=0 event=0 ip=198.41.192.107\n",
            "2026-02-26T12:27:28Z INF Retrying connection in up to 1s connIndex=0 event=0 ip=198.41.192.107\n",
            "2026-02-26T12:27:28Z ERR Connection terminated connIndex=0\n",
            "2026-02-26T12:27:28Z ERR no more connections active and exiting\n",
            "2026-02-26T12:27:28Z INF Tunnel server stopped\n",
            "2026-02-26T12:27:28Z ERR failed to run the datagram handler error=\"Application error 0x0 (remote)\" connIndex=0 event=0 ip=198.41.192.27\n",
            "2026-02-26T12:27:28Z INF Metrics server stopped\n",
            "2026-02-26T12:27:28Z ERR failed to serve tunnel connection error=\"accept stream listener encountered a failure while serving\" connIndex=0 event=0 ip=198.41.192.27\n",
            "2026-02-26T12:27:28Z ERR Serve tunnel error error=\"accept stream listener encountered a failure while serving\" connIndex=0 event=0 ip=198.41.192.27\n",
            "2026-02-26T12:27:28Z INF Retrying connection in up to 1s connIndex=0 event=0 ip=198.41.192.27\n",
            "2026-02-26T12:27:28Z ERR Connection terminated connIndex=0\n",
            "2026-02-26T12:27:28Z ERR no more connections active and exiting\n",
            "2026-02-26T12:27:28Z INF Tunnel server stopped\n",
            "2026-02-26T12:27:28Z INF Metrics server stopped\n",
            "Received second interrupt, terminating immediately.\n",
            "Received second interrupt, terminating immediately.\n"
          ]
        }
      ]
    }
  ]
}