{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMqPY3WMMdJ/1VFdbrU9Byi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiopauli/DDL/blob/main/Qwen27B_llamacpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Build llama.cpp with CUDA and run Qwen3.5-27B (non-thinking mode)\n",
        "!apt-get update -qq && apt-get install -qq -y pciutils build-essential cmake curl libcurl4-openssl-dev > /dev/null 2>&1\n",
        "\n",
        "!git clone --depth 1 https://github.com/ggml-org/llama.cpp 2>/dev/null || echo \"already cloned\"\n",
        "\n",
        "!cmake llama.cpp -B llama.cpp/build \\\n",
        "    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON > /dev/null 2>&1\n",
        "\n",
        "!cmake --build llama.cpp/build --config Release -j$(nproc) --clean-first --target llama-cli llama-server 2>&1 | tail -5\n",
        "\n",
        "!cp llama.cpp/build/bin/llama-* llama.cpp/\n",
        "\n",
        "# Download the model\n",
        "!pip install -q huggingface_hub hf_transfer\n",
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download unsloth/Qwen3.5-27B-GGUF \\\n",
        "    --local-dir unsloth/Qwen3.5-27B-GGUF \\\n",
        "    --include \"*UD-Q4_K_XL*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAd6icy52eED",
        "outputId": "1d5bd207-a6c4-4a84-b71d-6ee78f6ddde5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\n",
            "[100%] Linking CXX executable ../../bin/llama-server\n",
            "[100%] Built target llama-server\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/bin/bash: line 1: huggingface-cli: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHHrg6yO2ahU",
        "outputId": "f698db6f-e949-4fdf-9cca-8beb459d47d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n",
            "common_download_file_single_online: no previous model file found unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_preset.ini\n",
            "common_download_file_single_online: HEAD failed, status: 404\n",
            "\u001b[0mno remote preset found, skipping\n",
            "common_download_file_single_online: using cached file (same etag): unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_Qwen3.5-27B-UD-Q4_K_XL.gguf\n",
            "common_download_file_single_online: using cached file (same etag): unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_mmproj-BF16.gguf\n",
            "\n",
            "Loading model... |\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b \b\n",
            "\n",
            "\n",
            "▄▄ ▄▄\n",
            "██ ██\n",
            "██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄\n",
            "██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██\n",
            "██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀\n",
            "                                    ██    ██\n",
            "                                    ▀▀    ▀▀\n",
            "\n",
            "build      : b1-c747294\n",
            "model      : unsloth/Qwen3.5-27B-GGUF:UD-Q4_K_XL\n",
            "modalities : text, vision\n",
            "\n",
            "available commands:\n",
            "  /exit or Ctrl+C     stop or exit\n",
            "  /regen              regenerate the last response\n",
            "  /clear              clear the chat history\n",
            "  /read               add a text file\n",
            "  /image <file>       add an image file\n",
            "\n",
            "\u001b[1m\u001b[32m\n",
            "> "
          ]
        }
      ],
      "source": [
        "# Cell 2: Interactive chat (non-thinking, text only)\n",
        "import os\n",
        "os.environ[\"LLAMA_CACHE\"] = \"unsloth/Qwen3.5-27B-GGUF\"\n",
        "\n",
        "!./llama.cpp/llama-cli \\\n",
        "    -hf unsloth/Qwen3.5-27B-GGUF:UD-Q4_K_XL \\\n",
        "    --ctx-size 16384 \\\n",
        "    -ngl 99 \\\n",
        "    --temp 0.7 \\\n",
        "    --top-p 0.8 \\\n",
        "    --top-k 20 \\\n",
        "    --min-p 0.00 \\\n",
        "    --chat-template-kwargs '{\"enable_thinking\": false}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Interactive chat (non-thinking, text only)\n",
        "!./llama.cpp/llama-cli \\\n",
        "    --model unsloth/Qwen3.5-27B-GGUF/unsloth_Qwen3.5-27B-GGUF_Qwen3.5-27B-UD-Q4_K_XL.gguf \\\n",
        "    --ctx-size 16384 \\\n",
        "    -ngl 99 \\\n",
        "    --temp 0.7 \\\n",
        "    --top-p 0.8 \\\n",
        "    --top-k 20 \\\n",
        "    --min-p 0.00 \\\n",
        "    --chat-template-kwargs '{\"enable_thinking\": false}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7IPJ2i18G9b",
        "outputId": "f5619614-9758-4304-ec09-4f1ed5190c0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}